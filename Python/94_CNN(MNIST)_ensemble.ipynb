{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 우리가 배운 내용을 기초로 tensorflow의 mnist example을 CNN으로 구현해보자! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## 1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "# 전처리 단계(결측치, 이상치, 정규화, feature engineering)\n",
    "# mnist 예제에서는 이미 전처리가 끝난 상태이기 때문에 따로 할게 없다\n",
    "\n",
    "## Model 정의 (Tensorflow graph 생성)\n",
    "tf.reset_default_graph()  # 그래프 reset\n",
    "\n",
    "# 2.1 placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "keep_rate = tf.placeholder(dtype=tf.float32)  # 과적합을 피하기 위해서 nodes를 끈다\n",
    "# tensorflow 버전에 따라서 dropout을 사용하는 속성이 변경됨 \n",
    "# 최신버전: rate = 0.8 : drop시키는 노드의 비율\n",
    "# 이전버전: keep_prob = 0.8 : 살리는 노드의 비율\n",
    "\n",
    "\n",
    "# 2.2 Convolution Layer\n",
    "# 입력 데이터는 4차원 \n",
    "x_img = tf.reshape(X,[-1,28,28,1])  # 28*28, 색 1개, -1: 몇개의 이미지가 있는지는 알아서 계산해라 \n",
    "# 1층\n",
    "W1 = tf.Variable(tf.random_normal([3,3,1,32])) # 3*3, 채널1개(색), 32개의 feature map 생김(->activation map)\n",
    "L1 = tf.nn.conv2d(x_img, W1, strides=[1,1,1,1],  # 가로방향1칸, 세로방향1칸 \n",
    "                 padding=\"SAME\") # 패딩 SAME이고 strides를 1,1라고 하면 크기 유지 \n",
    "L1 = tf.nn.relu(L1) # 값이 너무 커지는 것 방지 \n",
    "# pooling 넣어도 되고 안넣어도 되고\n",
    "#  pooling의 목적 -> 이미지 특징을 도드라지게 or 이미지 사이지를 줄이게  둘중하나\n",
    "L1 = tf.nn.max_pool(L1,ksize=[1,2,2,1],strides=[1,2,2,1],\n",
    "                   padding=\"SAME\")  # 이미지 사이즈가 절반으로 줄어들 것! \n",
    "## 나온 결과-> 32개의 채널, 이미지크기는 14\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([3,3,32,64]))  # 32개의 채널들어오고, 64개 채널 만들 것이다.\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1,1,1,1],   \n",
    "                 padding=\"SAME\") \n",
    "L2 = tf.nn.relu(L2) \n",
    "L2 = tf.nn.max_pool(L2,ksize=[1,2,2,1],strides=[1,2,2,1],\n",
    "                   padding=\"SAME\")    # 이미지 크기 절반으로\n",
    "## 나온 결과 -> 64개의 채널, 이미지크기는 7\n",
    "\n",
    "\n",
    "# 학습을 시작해보자!  -> 학습하려면 1차원 데이터로 바꿔야 한다(여러개니까 2차원)\n",
    "# 2.3 FC layer (모든 레이어에 있는 노드(로지스틱)들이 다음 노드들과 연결되어있는 neural network) (dense layer)\n",
    "L2 = tf.reshape(L2, [-1,7*7*64])\n",
    "W3 = tf.get_variable(\"weight3\",shape=[7*7*64,256],  # 256은 내가 임의로 만들어 준 것!\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256]), name=\"bias3\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(L2,W3)+b3)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob =keep_rate)  # 과적합 피하기 위해 \n",
    "\n",
    "W4 = tf.get_variable(\"weight4\",shape=[256,512],  \n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]), name=\"bias4\")\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W4)+b4)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob =keep_rate)  # 과적합 피하기 위해 \n",
    "\n",
    "W5 = tf.get_variable(\"weight4\",shape=[512,10],  \n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]), name=\"bias5\")\n",
    "\n",
    "logit = tf.matmul(layer2,W5)+b5\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# cost \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, \n",
    "                                                                 labels=Y))\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)  # Adam 쓰려면 learning rate 더 작게 잡아주는게 일반적!\n",
    "\n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습 - epoch, batch 등 \n",
    "\n",
    "# 정확도 측정\n",
    "\n",
    "# prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드를 더 나은 형태로 바꿔보자! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## 1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "## Model 정의 (Tensorflow graph 생성)\n",
    "tf.reset_default_graph()  # 그래프 reset\n",
    "\n",
    "# 2.1 placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "dropout_rate = tf.placeholder(dtype=tf.float32)  # 과적합을 피하기 위해서 nodes를 끈다\n",
    "\n",
    "# 2.2 Convolution Layer\n",
    "x_img = tf.reshape(X,[-1,28,28,1])  # 28*28, 색 1개, -1: 몇개의 이미지가 있는지는 알아서 계산해라 \n",
    "\n",
    "L1 = tf.layers.conv2d(inputs=x_img, filters=32,\n",
    "                     kernel_size=[3,3], padding=\"SAME\",\n",
    "                     strides=1,  # 가로 세로 방향 같아야 하므로 [1,1]을 줄여서 1로 쓸 수 있다. \n",
    "                     activation=tf.nn.relu)  #relu함수 이용해서 activation 시킬거다\n",
    "# -> 이전 코드\n",
    "# W1 = tf.Variable(tf.random_normal([3,3,1,32]))                     # filter\n",
    "# L1 = tf.nn.conv2d(x_img, W1, strides=[1,1,1,1], padding=\"SAME\")    # 합성곱                \n",
    "# L1 = tf.nn.relu(L1)                                                 # Relu\n",
    "\n",
    "L1 = tf.layers.max_pooling2d(inputs=L1, pool_size=[2,2],padding=\"SAME\", strides=2)\n",
    "# -> 이전 코드\n",
    "# L1 = tf.nn.max_pool(L1,ksize=[1,2,2,1],strides=[1,2,2,1], padding=\"SAME\")   # pooling 작업\n",
    "\n",
    "L1 = tf.layers.dropout(inputs=L1, rate=dropout_rate)\n",
    "# -> 이전 코드\n",
    "# L1 = tf.nn.max_pool(L1,ksize=[1,2,2,1],strides=[1,2,2,1], padding=\"SAME\")  \n",
    "\n",
    "L2 = tf.layers.conv2d(inputs=L1, filters=64,\n",
    "                     kernel_size=[3,3], padding=\"SAME\",\n",
    "                     strides=1, activation=tf.nn.relu) \n",
    "L2 = tf.layers.max_pooling2d(inputs=L2, pool_size=[2,2],padding=\"SAME\", strides=2)\n",
    "L2 = tf.layers.dropout(inputs=L2, rate=dropout_rate)\n",
    "\n",
    "\n",
    "\n",
    "# 2.3 FC layer -> dense layer \n",
    "\n",
    "L2 = tf.reshape(L2, [-1,7*7*64])\n",
    "dense1 = tf.layers.dense(inputs=L2, units=256,   #결과 256개 넘기겠다는 거는 logistic 256개 가지고 있다는것\n",
    "                         activation=tf.nn.relu)\n",
    "# -> 이전 코드\n",
    "# W3 = tf.get_variable(\"weight3\",shape=[7*7*64,256],  # 256은 내가 임의로 만들어 준 것!\n",
    "#                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "# b3 = tf.Variable(tf.random_normal([256]), name=\"bias3\")\n",
    "# _layer1 = tf.nn.relu(tf.matmul(L2,W3)+b3)                   # 행렬곱 -> dense에서 해줌\n",
    "dense1 = tf.nn.dropout(dense1, rate=dropout_rate)  \n",
    "    # 계산할 때 배제하는거 -> 중간중간 노드 꺼줘야한다->없어도 되긴 하지만 보통 쓰는게 좋다(overfitting 피하기 위해)\n",
    "\n",
    "dense2 = tf.layers.dense(inputs=dense1, units=128, activation=tf.nn.relu)\n",
    "dense2 = tf.nn.dropout(dense2, rate=dropout_rate)\n",
    "\n",
    "dense3 = tf.layers.dense(inputs=dense2, units=512, activation=tf.nn.relu)\n",
    "dense3 = tf.nn.dropout(dense3, rate=dropout_rate)\n",
    "\n",
    "H = tf.layers.dense(inputs=dense3, units=10)  # H에서는 activation 쓰지 않음\n",
    "# -> 이전 코드\n",
    "# W5 = tf.get_variable(\"weight4\",shape=[512,10],  \n",
    "#                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "# b5 = tf.Variable(tf.random_normal([10]), name=\"bias5\")\n",
    "# logit = tf.matmul(layer2,W5)+b5\n",
    "# H = tf.nn.softmax(logit)\n",
    "\n",
    "# cost \n",
    "cost = tf.losses.softmax_cross_entropy(Y,H)\n",
    "# -> 이전 코드\n",
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y))\n",
    "\n",
    "# train\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)  # 여기서는 adam쓰는게 일반적\n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습 - epoch, batch 등 \n",
    "\n",
    "\n",
    "# 정확도 측정\n",
    "\n",
    "# prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 위 코드 주석 빼고 적어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## 1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "## Model 정의 (Tensorflow graph 생성)\n",
    "tf.reset_default_graph()  # 그래프 reset\n",
    "\n",
    "# 2.1 placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "dropout_rate = tf.placeholder(dtype=tf.float32)  # 과적합을 피하기 위해서 nodes를 끈다\n",
    "\n",
    "# 2.2 Convolution Layer\n",
    "x_img = tf.reshape(X,[-1,28,28,1])  # 28*28, 색 1개, -1: 몇개의 이미지가 있는지는 알아서 계산해라 \n",
    "\n",
    "L1 = tf.layers.conv2d(inputs=x_img, filters=32, kernel_size=[3,3], padding=\"SAME\",\n",
    "                     strides=1,activation=tf.nn.relu)  \n",
    "L1 = tf.layers.max_pooling2d(inputs=L1, pool_size=[2,2],padding=\"SAME\", strides=2)\n",
    "L1 = tf.layers.dropout(inputs=L1, rate=dropout_rate)\n",
    "\n",
    "L2 = tf.layers.conv2d(inputs=L1, filters=64, kernel_size=[3,3], padding=\"SAME\",\n",
    "                     strides=1, activation=tf.nn.relu) \n",
    "L2 = tf.layers.max_pooling2d(inputs=L2, pool_size=[2,2],padding=\"SAME\", strides=2)\n",
    "L2 = tf.layers.dropout(inputs=L2, rate=dropout_rate)\n",
    "\n",
    "# 2.3 FC layer -> dense layer \n",
    "L2 = tf.reshape(L2, [-1,7*7*64])\n",
    "dense1 = tf.layers.dense(inputs=L2, units=256, activation=tf.nn.relu)\n",
    "dense1 = tf.nn.dropout(dense1, rate=dropout_rate)  \n",
    "\n",
    "dense2 = tf.layers.dense(inputs=dense1, units=128, activation=tf.nn.relu)\n",
    "dense2 = tf.nn.dropout(dense2, rate=dropout_rate)\n",
    "\n",
    "dense3 = tf.layers.dense(inputs=dense2, units=512, activation=tf.nn.relu)\n",
    "dense3 = tf.nn.dropout(dense3, rate=dropout_rate)\n",
    "\n",
    "H = tf.layers.dense(inputs=dense3, units=10)  # H에서는 activation 쓰지 않음\n",
    "\n",
    "# cost \n",
    "cost = tf.losses.softmax_cross_entropy(Y,H)\n",
    "\n",
    "# train\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost) \n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습 - epoch, batch 등 \n",
    "\n",
    "\n",
    "# 정확도 측정\n",
    "\n",
    "# prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결국 우리의 MNIST 예제는 multinomial 예제! \n",
    "### 이미지 1개에 대한 예측값의 H의 도출값은 [0.5, 0.3, 0.2, 0.001, 0.99, 0.44, ...] \n",
    "-> 각각 0이 될 확률, 1이 될 확률 등 10개   \n",
    "가장 큰 곳의 숫자가 예측 숫자가 될 것이다.  \n",
    "학습 결과는 학습할때마다 달라짐 (초기값 랜덤이라)\n",
    "\n",
    "## 앙상블(ensemble)은 모델을 여러개 만드는 것! (10개의 모델)\n",
    "\n",
    "우리가 지금까지 한 것은 한 개의 모델 만드는 과정. \n",
    "이미지 1개에 대한 각 모델의 예측값   \n",
    "Model1(H1) : [0.5, 0.3, 0.02, 0.001, 0.099, 0.044, ...]   \n",
    "Model2(H2) : [0.4, 0.2, 0.03, 0.011, 0.090, 0.064, ...]   \n",
    "...\n",
    "=> SUM => [0.9, 0.5, 0.05, ...]  \n",
    "각 자리별로 다 더해서 sum 구한다.  \n",
    "이 SUM을 가지고 prediction 한다.   => 이 값은 0일 것이다. \n",
    "\n",
    "즉, 한 사람의 전문가에게 의견을 구하는 것이 아니라 여러 사람의 전문가에게 의견을 구해서 가장 그럴듯한 답을 찾아내는 방식이라고 할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그냥 한번 돌려보자 (앙상블 X)\n",
    "# 런타임 -> GPU\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## 1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "## Model 정의 (Tensorflow graph 생성)\n",
    "tf.reset_default_graph()  # 그래프 reset\n",
    "\n",
    "# 2.1 placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "dropout_rate = tf.placeholder(dtype=tf.float32)  # 과적합을 피하기 위해서 nodes를 끈다\n",
    "\n",
    "# 2.2 Convolution Layer\n",
    "x_img = tf.reshape(X,[-1,28,28,1])  # 28*28, 색 1개, -1: 몇개의 이미지가 있는지는 알아서 계산해라 \n",
    "\n",
    "L1 = tf.layers.conv2d(inputs=x_img, filters=32, kernel_size=[3,3], padding=\"SAME\",\n",
    "                     strides=1,activation=tf.nn.relu)  \n",
    "L1 = tf.layers.max_pooling2d(inputs=L1, pool_size=[2,2],padding=\"SAME\", strides=2)\n",
    "L1 = tf.layers.dropout(inputs=L1, rate=dropout_rate)\n",
    "\n",
    "L2 = tf.layers.conv2d(inputs=L1, filters=64, kernel_size=[3,3], padding=\"SAME\",\n",
    "                     strides=1, activation=tf.nn.relu) \n",
    "L2 = tf.layers.max_pooling2d(inputs=L2, pool_size=[2,2],padding=\"SAME\", strides=2)\n",
    "L2 = tf.layers.dropout(inputs=L2, rate=dropout_rate)\n",
    "\n",
    "# 2.3 FC layer -> dense layer \n",
    "L2 = tf.reshape(L2, [-1,7*7*64])\n",
    "dense1 = tf.layers.dense(inputs=L2, units=256, activation=tf.nn.relu)\n",
    "dense1 = tf.nn.dropout(dense1, rate=dropout_rate)  \n",
    "\n",
    "dense2 = tf.layers.dense(inputs=dense1, units=128, activation=tf.nn.relu)\n",
    "dense2 = tf.nn.dropout(dense2, rate=dropout_rate)\n",
    "\n",
    "dense3 = tf.layers.dense(inputs=dense2, units=512, activation=tf.nn.relu)\n",
    "dense3 = tf.nn.dropout(dense3, rate=dropout_rate)\n",
    "\n",
    "H = tf.layers.dense(inputs=dense3, units=10)  # H에서는 activation 쓰지 않음\n",
    "\n",
    "# cost \n",
    "cost = tf.losses.softmax_cross_entropy(Y,H)\n",
    "\n",
    "# train\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost) \n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습 - epoch, batch 등 \n",
    "num_of_epoch = 30\n",
    "batch_size = 100\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)\n",
    "    cost_val = 0\n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)        \n",
    "        _, cost_val = sess.run([train,cost],feed_dict = {X:batch_x,\n",
    "                                                       Y:batch_y,\n",
    "                                                       dropout_rate:0.3}) # 30% 끄고 학습해라\n",
    "    if step%3==0:\n",
    "        print(\"cost:{}\".format(cost_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy 측정\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "print(\"정확도:{}\".format(sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                                       Y:mnist.test.labels,\n",
    "                                                       dropout_rate:0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "result = sess.run(H, feed_dict={X:test_df})\n",
    "\n",
    "predict = tf.cast(result > 0.5,dtype=tf.int32)\n",
    "result = sess.run(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앙상블 해보자. 에러!!@!@!@\n",
    "# 런타임 -> GPU\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## 1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "## Model 정의 (Tensorflow graph 생성)\n",
    "tf.reset_default_graph()  # 그래프 reset\n",
    "\n",
    "# 2.1 placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "dropout_rate = tf.placeholder(dtype=tf.float32)  # 과적합을 피하기 위해서 nodes를 끈다\n",
    "\n",
    "# 2.2 Convolution Layer\n",
    "x_img = tf.reshape(X,[-1,28,28,1])  # 28*28, 색 1개, -1: 몇개의 이미지가 있는지는 알아서 계산해라 \n",
    "\n",
    "L1 = tf.layers.conv2d(inputs=x_img, filters=32, kernel_size=[3,3], padding=\"SAME\",\n",
    "                strides=1,activation=tf.nn.relu)  \n",
    "L1 = tf.layers.max_pooling2d(inputs=L1, pool_size=[2,2],padding=\"SAME\", strides=2)\n",
    "L1 = tf.layers.dropout(inputs=L1, rate=dropout_rate)\n",
    "\n",
    "L2 = tf.layers.conv2d(inputs=L1, filters=64, kernel_size=[3,3], padding=\"SAME\",\n",
    "                strides=1, activation=tf.nn.relu) \n",
    "L2 = tf.layers.max_pooling2d(inputs=L2, pool_size=[2,2],padding=\"SAME\", strides=2)\n",
    "L2 = tf.layers.dropout(inputs=L2, rate=dropout_rate)\n",
    "\n",
    "# 2.3 FC layer -> dense layer \n",
    "L2 = tf.reshape(L2, [-1,7*7*64])\n",
    "dense1 = tf.layers.dense(inputs=L2, units=256, activation=tf.nn.relu)\n",
    "dense1 = tf.nn.dropout(dense1, rate=dropout_rate)  \n",
    "\n",
    "dense2 = tf.layers.dense(inputs=dense1, units=512, activation=tf.nn.relu)\n",
    "dense2 = tf.nn.dropout(dense2, rate=dropout_rate)\n",
    "\n",
    "H = tf.layers.dense(inputs=dense2, units=10)  # H에서는 activation 쓰지 않음\n",
    "\n",
    "# cost \n",
    "cost = tf.losses.softmax_cross_entropy(Y,H)\n",
    "\n",
    "# train\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost) \n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# 학습 - epoch, batch 등 \n",
    "\n",
    "for n in range(2):\n",
    "    # 그래프 reset\n",
    "    tf.reset_default_graph() \n",
    "\n",
    "    num_of_epoch = 5\n",
    "    batch_size = 100\n",
    "\n",
    "    # 모델은 생성됐으니까 H만 돌려보자.\n",
    "    for step in range(num_of_epoch):\n",
    "        num_of_iter = int(mnist.train.num_examples/batch_size)\n",
    "        cost_val = 0\n",
    "        for i in range(num_of_iter):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)        \n",
    "            _, cost_val,H_val = sess.run([train,cost,H],feed_dict = {X:batch_x,\n",
    "                                                        Y:batch_y,\n",
    "                                                        dropout_rate:0.3}) # 30% 끄고 학습해라\n",
    "    print(cost_val)\n",
    "    # test 데이터로 돌려주자 \n",
    "    H_val = sess.run(H, feed_dict={X:mnist.test.images})\n",
    "\n",
    "    # 나온 H 값들을 각 자리마다 더해준다.\n",
    "    if n==0:  # 초기 ndarray 지정\n",
    "        H_result = H_val\n",
    "    else:\n",
    "        H_result = np.add(H_result,H_val)  # 각 자리의 합을 구해주는 함수\n",
    "    \n",
    "predict_result = np.argmax(H_result,1)\n",
    "true_result = np.argmax(mnist.test.labels,1)\n",
    "correct = np.equal(predict_result,true_result)\n",
    "accuracy = np.mean(correct)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 준원 ####\n",
    "\n",
    "\n",
    "# Model 정의(Tensorflow graph 생성)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 2.1 placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "dropout_rate = tf.placeholder(dtype = tf.float32)\n",
    "\n",
    "\n",
    "# 2.2 FC Layer(dense layer)\n",
    "# 입력데이터는 4차원\n",
    "x_img = tf.reshape(X, [-1,28,28,1])\n",
    "L1 = tf.layers.conv2d(inputs = x_img,\n",
    "                      filters = 32,\n",
    "                      kernel_size = [3,3],\n",
    "                      padding = \"SAME\",\n",
    "                      strides = 1,\n",
    "                      activation = tf.nn.relu)\n",
    "\n",
    "L1 = tf.layers.max_pooling2d(inputs = L1, \n",
    "                             pool_size = [2,2], \n",
    "                             padding = \"SAME\",\n",
    "                             strides = 2)\n",
    "\n",
    "L1 = tf.layers.dropout(inputs = L1,\n",
    "                       rate = dropout_rate)\n",
    "\n",
    "\n",
    "L2 = tf.layers.conv2d(inputs = L1,\n",
    "                      filters = 64,\n",
    "                      kernel_size = [3,3],\n",
    "                      padding = \"SAME\",\n",
    "                      strides = 1,\n",
    "                      activation = tf.nn.relu)\n",
    "\n",
    "L2 = tf.layers.max_pooling2d(inputs = L2, \n",
    "                             pool_size = [2,2], \n",
    "                             padding = \"SAME\",\n",
    "                             strides = 2)\n",
    "\n",
    "L2 = tf.layers.dropout(inputs = L2,\n",
    "                       rate = dropout_rate)\n",
    "\n",
    "\n",
    "\n",
    "# 2.3 FC Layer (dense layer)\n",
    "L2 = tf.reshape(L2, [-1,7*7*64])\n",
    "\n",
    "dense1 = tf.layers.dense(inputs = L2,\n",
    "                         units = 256,\n",
    "                         activation=tf.nn.relu)\n",
    "\n",
    "dense1 = tf.layers.dropout(inputs = dense1,\n",
    "                           rate = dropout_rate)\n",
    "\n",
    "\n",
    "dense2 = tf.layers.dense(inputs = dense1,\n",
    "                         units = 128,\n",
    "                         activation=tf.nn.relu)\n",
    "\n",
    "dense2 = tf.layers.dropout(inputs = dense2,\n",
    "                           rate = dropout_rate)\n",
    "\n",
    "\n",
    "dense3 = tf.layers.dense(inputs = dense2,\n",
    "                         units = 512,\n",
    "                         activation=tf.nn.relu)\n",
    "\n",
    "dense3 = tf.layers.dropout(inputs = dense3,\n",
    "                           rate = dropout_rate)\n",
    "\n",
    "\n",
    "H = tf.layers.dense(inputs=dense3, units=10)\n",
    "\n",
    "\n",
    "# cost\n",
    "cost = tf.losses.softmax_cross_entropy(Y,H)\n",
    "\n",
    "# train\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "# session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10):\n",
    "    tf.reset_default_graph()\n",
    "    num_of_epoch = 30\n",
    "    batch_size = 100\n",
    "    \n",
    "    for step in range(num_of_epoch):\n",
    "        num_of_iter = int(mnist.train.num_examples / batch_size)\n",
    "        cost_val = 0\n",
    "        for j in range(num_of_iter):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            _,H_val,cost_val = sess.run([train,H,cost], feed_dict={X : batch_x,\n",
    "                                                       Y : batch_y, \n",
    "                                                       dropout_rate : 0.3})\n",
    "    print(cost_val)    \n",
    "    H_val = sess.run(H, feed_dict={X: mnist.test.images})\n",
    "    if i == 0:\n",
    "        H_result = H_val\n",
    "    else:\n",
    "        H_result = np.add(H_result,H_val)\n",
    "\n",
    "predict = np.argmax(H_result,1)\n",
    "Y = mnist.test.labels\n",
    "correct = np.equal(predict, np.argmax(Y,1))\n",
    "accuracy = np.mean(correct)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 지수 ####\n",
    "\n",
    "# 런타임 -> GPU\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## 1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "H_list = []\n",
    "\n",
    "for p in range(10):\n",
    "    ## Model 정의 (Tensorflow graph 생성)\n",
    "    tf.reset_default_graph()  # 그래프 reset\n",
    "\n",
    "    # 2.1 placeholder\n",
    "    X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "    Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "    dropout_rate = tf.placeholder(dtype=tf.float32)  # 과적합을 피하기 위해서 nodes를 끈다\n",
    "\n",
    "    # 2.2 Convolution Layer\n",
    "    x_img = tf.reshape(X,[-1,28,28,1])  # 28*28, 색 1개, -1: 몇개의 이미지가 있는지는 알아서 계산해라 \n",
    "\n",
    "    L1 = tf.layers.conv2d(inputs=x_img, filters=32, kernel_size=[3,3], padding=\"SAME\",\n",
    "                        strides=1,activation=tf.nn.relu)  \n",
    "    L1 = tf.layers.max_pooling2d(inputs=L1, pool_size=[2,2],padding=\"SAME\", strides=2)\n",
    "    L1 = tf.layers.dropout(inputs=L1, rate=dropout_rate)\n",
    "\n",
    "    L2 = tf.layers.conv2d(inputs=L1, filters=64, kernel_size=[3,3], padding=\"SAME\",\n",
    "                        strides=1, activation=tf.nn.relu) \n",
    "    L2 = tf.layers.max_pooling2d(inputs=L2, pool_size=[2,2],padding=\"SAME\", strides=2)\n",
    "    L2 = tf.layers.dropout(inputs=L2, rate=dropout_rate)\n",
    "\n",
    "    # 2.3 FC layer -> dense layer \n",
    "    L2 = tf.reshape(L2, [-1,7*7*64])\n",
    "    dense1 = tf.layers.dense(inputs=L2, units=256, activation=tf.nn.relu)\n",
    "    dense1 = tf.nn.dropout(dense1, rate=dropout_rate)  \n",
    "\n",
    "    dense2 = tf.layers.dense(inputs=dense1, units=128, activation=tf.nn.relu)\n",
    "    dense2 = tf.nn.dropout(dense2, rate=dropout_rate)\n",
    "\n",
    "    dense3 = tf.layers.dense(inputs=dense2, units=512, activation=tf.nn.relu)\n",
    "    dense3 = tf.nn.dropout(dense3, rate=dropout_rate)\n",
    "\n",
    "    H = tf.layers.dense(inputs=dense3, units=10)  # H에서는 activation 쓰지 않음\n",
    "\n",
    "    # cost \n",
    "    cost = tf.losses.softmax_cross_entropy(Y,H)\n",
    "\n",
    "    # train\n",
    "    train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost) \n",
    "\n",
    "    # Session & 초기화\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # 학습 - epoch, batch 등 \n",
    "    num_of_epoch = 30\n",
    "    batch_size = 100\n",
    "\n",
    "    for step in range(num_of_epoch):\n",
    "        num_of_iter = int(mnist.train.num_examples/batch_size)\n",
    "        cost_val = 0\n",
    "\n",
    "        for i in range(num_of_iter):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)        \n",
    "            _, cost_val = sess.run([train,cost],feed_dict = {X:batch_x,\n",
    "                                                             Y:batch_y,\n",
    "                                                             dropout_rate:0.3}) # 30% 끄고 학습해라\n",
    "    result = sess.run(H, feed_dict={X: mnist.test.images,\n",
    "                                    Y: mnist.test.labels,\n",
    "                                    dropout_rate: 0})\n",
    "    print(result)\n",
    "    H_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_array = np.array(H_list)\n",
    "print(H_array.shape)\n",
    "\n",
    "k = 0\n",
    "for w in range(10):\n",
    "    t = H_array[w]\n",
    "    k = k + t\n",
    "print (k.shape)  \n",
    "\n",
    "predict = tf.argmax(k,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32 ))\n",
    "\n",
    "sess.run(accuracy, feed_dict = {X: mnist.test.images,\n",
    "                                Y: mnist.test.labels,\n",
    "                                dropout_rate: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[CPU_ENV]",
   "language": "python",
   "name": "cpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
