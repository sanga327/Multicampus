{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## digit-recognizer 데이터를 이용하여 앙상블 해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#import mglearn \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train_df = pd.read_csv(\"./data/digit-recognizer/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/digit-recognizer/test.csv\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test 데이터 생성\n",
    "# 정규화\n",
    "scaler= MinMaxScaler()\n",
    "#train_num = int(train_df.shape[0]*0.8) \n",
    "train_num = int(train_df.shape[0])\n",
    "\n",
    "x_data = train_df.drop(\"label\",axis=1,inplace=False)\n",
    "x_data = scaler.fit_transform(x_data)\n",
    "\n",
    "sess = tf.Session()\n",
    "y_data = sess.run(tf.one_hot(train_df[\"label\"], 10))\n",
    "\n",
    "# 데이터 분할해 x data 생성 - 80:20\n",
    "train_x_data = x_data[:train_num]  # nparray에서 자른 값 ->  뒤의 값이 exclusive 해서 16000개\n",
    "test_x_data = x_data[train_num:]\n",
    "\n",
    "# 학습용, 테스트용 y data 생성 -> one hot encoding\n",
    "\n",
    "train_y_data = sess.run(tf.one_hot(train_df.loc[:train_num-1,\"label\"], 10)) # 원래 1차원 자료가 one-hot encoding 적용하면 2차원 자료가 된다.\n",
    "# sess.run(tf.one_hot(bmi_df.loc[:train_num-1,\"label\"], 3)).shape # data frame이라 loc 사용하면 뒤부분 inclusive하다 -> train_num -1 빼줘야 함! \n",
    "test_y_data = sess.run(tf.one_hot(train_df.loc[train_num:,\"label\"], 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dropout() got an unexpected keyword argument 'rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c371ea602fd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mL2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mdense1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mL2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mdense1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdense1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mdense2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdense1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: dropout() got an unexpected keyword argument 'rate'"
     ]
    }
   ],
   "source": [
    "\n",
    "## Model 정의 (Tensorflow graph 생성)\n",
    "tf.reset_default_graph()  # 그래프 reset\n",
    "\n",
    "# 2.1 placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "dropout_rate = tf.placeholder(dtype=tf.float32)  # 과적합을 피하기 위해서 nodes를 끈다\n",
    "\n",
    "# 2.2 Convolution Layer\n",
    "x_img = tf.reshape(X,[-1,28,28,1])  # 28*28, 색 1개, -1: 몇개의 이미지가 있는지는 알아서 계산해라 \n",
    "\n",
    "L1 = tf.layers.conv2d(inputs=x_img, filters=32, kernel_size=[3,3], padding=\"SAME\",\n",
    "                strides=1,activation=tf.nn.relu)  \n",
    "L1 = tf.layers.max_pooling2d(inputs=L1, pool_size=[2,2],padding=\"SAME\", strides=2)\n",
    "L1 = tf.layers.dropout(inputs=L1, rate=dropout_rate)\n",
    "\n",
    "L2 = tf.layers.conv2d(inputs=L1, filters=64, kernel_size=[3,3], padding=\"SAME\",\n",
    "                strides=1, activation=tf.nn.relu) \n",
    "L2 = tf.layers.max_pooling2d(inputs=L2, pool_size=[2,2],padding=\"SAME\", strides=2)\n",
    "L2 = tf.layers.dropout(inputs=L2, rate=dropout_rate)\n",
    "\n",
    "# 2.3 FC layer -> dense layer \n",
    "L2 = tf.reshape(L2, [-1,7*7*64])\n",
    "dense1 = tf.layers.dense(inputs=L2, units=256, activation=tf.nn.relu)\n",
    "dense1 = tf.nn.dropout(dense1, rate=dropout_rate)  \n",
    "\n",
    "dense2 = tf.layers.dense(inputs=dense1, units=512, activation=tf.nn.relu)\n",
    "dense2 = tf.nn.dropout(dense2, rate=dropout_rate)\n",
    "\n",
    "H = tf.layers.dense(inputs=dense2, units=10)  # H에서는 activation 쓰지 않음\n",
    "\n",
    "# cost \n",
    "cost = tf.losses.softmax_cross_entropy(Y,H)\n",
    "\n",
    "# train\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost) \n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# 학습 - epoch, batch 등 \n",
    "\n",
    "for n in range(15):\n",
    "    # 그래프 reset\n",
    "    tf.reset_default_graph() \n",
    "\n",
    "    num_of_epoch = 30\n",
    "    batch_size = 100\n",
    "\n",
    "    # 모델은 생성됐으니까 H만 돌려보자.\n",
    "    for step in range(num_of_epoch):\n",
    "        num_of_iter = int(train_num/batch_size)\n",
    "        cost_val = 0\n",
    "        for i in range(num_of_iter):\n",
    "            batch_x = train_x_data[batch_size*i:batch_size*(i+1)]\n",
    "            batch_y = train_y_data[batch_size*i:batch_size*(i+1)]       \n",
    "            _, cost_val,H_val = sess.run([train,cost,H],feed_dict = {X:batch_x,\n",
    "                                                        Y:batch_y,\n",
    "                                                        dropout_rate:0.3}) # 30% 끄고 학습해라\n",
    "    print(cost_val)\n",
    "    # test 데이터로 돌려주자 \n",
    "    H_val = sess.run(H, feed_dict={X:test_x_data,\n",
    "                                   dropout_rate:0.3})\n",
    "\n",
    "    # 나온 H 값들을 각 자리마다 더해준다.\n",
    "    if n==0:  # 초기 ndarray 지정\n",
    "        H_result = H_val\n",
    "    else:\n",
    "        H_result = np.add(H_result,H_val)  # 각 자리의 합을 구해주는 함수\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 종료되었으니 정확도를 측정하자! \n",
    "\n",
    "predict_result = np.argmax(H_result,1)\n",
    "true_result = np.argmax(test_y_data,1)\n",
    "correct = np.equal(predict_result,true_result)\n",
    "accuracy = np.mean(correct)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체를 학습해서 test데이터로 돌려보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#import mglearn \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train_df = pd.read_csv(\"/content/gdrive/My Drive/python_DA/data/digit-recognizer/train.csv\")\n",
    "test_df = pd.read_csv(\"/content/gdrive/My Drive/python_DA/data/digit-recognizer/test.csv\") \n",
    "\n",
    "# 학습용 데이터\n",
    "scaler= MinMaxScaler()\n",
    "#train_num = int(train_df.shape[0]*0.8) \n",
    "train_num = int(train_df.shape[0])\n",
    "\n",
    "# train용 데이터\n",
    "train_x_data = train_df.drop(\"label\",axis=1,inplace=False)\n",
    "train_x_data = scaler.fit_transform(train_x_data)\n",
    "\n",
    "sess = tf.Session()\n",
    "train_y_data = sess.run(tf.one_hot(train_df[\"label\"], 10))\n",
    "\n",
    "# test용 데이터\n",
    "test_x_data = test_df\n",
    "test_x_data = scaler.fit_transform(test_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 정의 (Tensorflow graph 생성)\n",
    "tf.reset_default_graph()  # 그래프 reset\n",
    "\n",
    "# 2.1 placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "dropout_rate = tf.placeholder(dtype=tf.float32)  # 과적합을 피하기 위해서 nodes를 끈다\n",
    "\n",
    "# 2.2 Convolution Layer\n",
    "x_img = tf.reshape(X,[-1,28,28,1])  # 28*28, 색 1개, -1: 몇개의 이미지가 있는지는 알아서 계산해라 \n",
    "\n",
    "L1 = tf.layers.conv2d(inputs=x_img, filters=32, kernel_size=[3,3], padding=\"SAME\",\n",
    "                strides=1,activation=tf.nn.relu)  \n",
    "L1 = tf.layers.max_pooling2d(inputs=L1, pool_size=[2,2],padding=\"SAME\", strides=2)\n",
    "L1 = tf.layers.dropout(inputs=L1, rate=dropout_rate)\n",
    "\n",
    "L2 = tf.layers.conv2d(inputs=L1, filters=64, kernel_size=[3,3], padding=\"SAME\",\n",
    "                strides=1, activation=tf.nn.relu) \n",
    "L2 = tf.layers.max_pooling2d(inputs=L2, pool_size=[2,2],padding=\"SAME\", strides=2)\n",
    "L2 = tf.layers.dropout(inputs=L2, rate=dropout_rate)\n",
    "\n",
    "# 2.3 FC layer -> dense layer \n",
    "L2 = tf.reshape(L2, [-1,7*7*64])\n",
    "dense1 = tf.layers.dense(inputs=L2, units=256, activation=tf.nn.relu)\n",
    "dense1 = tf.nn.dropout(dense1, rate=dropout_rate)  \n",
    "\n",
    "dense2 = tf.layers.dense(inputs=dense1, units=512, activation=tf.nn.relu)\n",
    "dense2 = tf.nn.dropout(dense2, rate=dropout_rate)\n",
    "\n",
    "H = tf.layers.dense(inputs=dense2, units=10)  # H에서는 activation 쓰지 않음\n",
    "\n",
    "# cost \n",
    "cost = tf.losses.softmax_cross_entropy(Y,H)\n",
    "\n",
    "# train\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost) \n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# 학습 - epoch, batch 등 \n",
    "\n",
    "for n in range(10):\n",
    "    # 그래프 reset\n",
    "    tf.reset_default_graph() \n",
    "\n",
    "    num_of_epoch = 30\n",
    "    batch_size = 100\n",
    "\n",
    "    # 모델은 생성됐으니까 H만 돌려보자.\n",
    "    for step in range(num_of_epoch):\n",
    "        num_of_iter = int(train_num/batch_size)\n",
    "        cost_val = 0\n",
    "        for i in range(num_of_iter):\n",
    "            batch_x = train_x_data[batch_size*i:batch_size*(i+1)]\n",
    "            batch_y = train_y_data[batch_size*i:batch_size*(i+1)]       \n",
    "            _, cost_val,H_val = sess.run([train,cost,H],feed_dict = {X:batch_x,\n",
    "                                                                     Y:batch_y,\n",
    "                                                                     dropout_rate:0.3}) # 30% 끄고 학습해라\n",
    "    #print(cost_val)\n",
    "    # test 데이터로 돌려주자 \n",
    "    H_val = sess.run(H, feed_dict={X:test_x_data,\n",
    "                                   dropout_rate:0})\n",
    "\n",
    "    # 나온 H 값들을 각 자리마다 더해준다.\n",
    "    if n==0:  # 초기 ndarray 지정\n",
    "        H_result = H_val\n",
    "    else:\n",
    "        H_result = np.add(H_result,H_val)  # 각 자리의 합을 구해주는 함수\n",
    "    \n",
    "predict_result = np.argmax(H_result,1)\n",
    "print(predict_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = pd.DataFrame()\n",
    "my_df[\"ImageId\"] = range(1,test_df.shape[0]+1)\n",
    "my_df[\"Label\"] = predict_result\n",
    "my_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.to_csv(\"/content/gdrive/My Drive/python_DA/data/digit-recognizer/ensemble.csv\",\n",
    "                 sep=',',\n",
    "                 na_rep='NaN', \n",
    "                 columns = ['ImageId','Label'], # columns to write\n",
    "                 index = False) # do not write index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[CPU_ENV]",
   "language": "python",
   "name": "cpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
