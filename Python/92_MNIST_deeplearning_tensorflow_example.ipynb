{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 소프트웨어 측면 - 인공지능(AI)\n",
    "- 하드웨어 측면 - 양자이론, 양자컴퓨터\n",
    "\n",
    "### 빅뱅의 시작을 1년으로 잡으면 \n",
    "- 인류의 탄생은 2일 전 (호모사피엔스)\n",
    "- 산업혁명은 2초 전 \n",
    "- 기술의 발전 속도는 기하급수적으로 증가하고 있음 \n",
    "- 언젠가는 우리가 만드는 프로그램이 사람의 지능을 앞서는 순간이 올 것이라고 예측할 수 있다! \n",
    "- 이 시점을 특이점(singlurarity)라고 한다.\n",
    "- 조만간 특이점이 올 것이라고 많은 사람들이 생각하고 있고 그 시점을 사람들이 예측해보면 2045년도일 것...\n",
    "- 많은 학자들 중 일부는 특이점이 오는 시기가 인류가 멸망하는 시기라고 예측\n",
    "- 회사들은 안전하다고 말함 > AI가 선 악을 구분할 수 있을 것이다. 그래서 사람을 죽이거나 하지 않을 것이다. \n",
    "- 프로그래밍을 통해 AI를 제어할 수 있음\n",
    "\n",
    "###  뇌과학자 => AI가 개발이 되면 인간과 학습하는 능력이 비슷할 것인데, 속도는 차이남.\n",
    "- 인공지능은 전자회로의 속도로 학습을 하는 것이고 사람은 생화학적 회로로 학습\n",
    "- 전자회로의 속도가 약 100만배정도 빠름\n",
    "- MIT의 AI 개발자들이 만약 인공지능을 만들어 내면 인공지능이 1주일동안 할 수 있는 일을 MIT AI 개발자들이 2만년이 걸린다... > 인공지능이 우리를 미개하게 볼 것 > 사람을 죽이지 말라는 프로그래밍 코드를 삽입해도 의미없음\n",
    "\n",
    "### 현 시점에 가장 빠른 슈퍼컴퓨터: 미국-IBM이 만든 서밋(summit)\n",
    "- 농구코트의 2배정도 되는 크기에 캐비넷 깔아놓고 그 안에 컴퓨터를 채워넣음 \n",
    "- 기후예측, 화성탐사선 시뮬레이션(우주개발 시뮬레이션) 등을 수행함\n",
    "\n",
    "### 작년 10월 23일 구글이 네이처지에 양자컴퓨터 개발에 대한 논문 발표\n",
    "- 서밋이 1000년동안 해야 하는 일을 3.5초만에 해결했다고 발표 -> 사실과는 다름 \n",
    "- IBM의 반격: 서밋이 2.5일 정도 되는 일을 3.5초만에 해결 (그래도 빠르긴 함)\n",
    "- 양자컴퓨터는 모든 산업에 적용되지 않고 특정 분야에만 적용 가능 - 인간의 게놈 지도, 양자연구 등\n",
    "- 기존의 보안 체계에는 문제 없을 것! \n",
    "\n",
    "\n",
    "\n",
    "# 인공지능(AI)\n",
    "\n",
    "- CS에서 궁극의 목표 중 하나 \n",
    "- 문제도 많음\n",
    "- 직업적인 문제가 생길 수 있음\n",
    "- 20년 뒤에는 직업의 절반 가량이 없어지고 10년 더 지나면 모든 직업이 사라질 것이라는 예측이 있다. \n",
    "- 사람은 무엇을 하며 살아야 하는가?\n",
    "\n",
    "- 인간의 뇌를 연구하기 시작\n",
    "- perceptron: 사람의 뇌처럼 모델링 한 것\n",
    "- 1958년에 perceptron을 모델링한 기계를 실제로 구현\n",
    "- 뉴욕타임즈에 기사가 실림 \n",
    "    - 조금만 있으면 스스로 말하고, 듣고, 쓰고, 창조가 가능한 프로그램을 만들 수 있다.\n",
    "- AND/OR에 대한 logistic regression => perceptron\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost값은:0.6766015291213989\n",
      "cost값은:0.3063524663448334\n",
      "cost값은:0.20591184496879578\n",
      "cost값은:0.1560397893190384\n",
      "cost값은:0.12551918625831604\n",
      "cost값은:0.10480726510286331\n",
      "cost값은:0.08982474356889725\n",
      "cost값은:0.07849374413490295\n",
      "cost값은:0.06963463127613068\n",
      "cost값은:0.06252612173557281\n"
     ]
    }
   ],
   "source": [
    "# tensorflow로 logistic 구현해보자!\n",
    "# 진리표를 이용해서 학습할 것\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# AND 연산\n",
    "x_data = [[0,0],\n",
    "         [0,1],\n",
    "         [1,0],\n",
    "         [1,1]]\n",
    "y_data = [[0],\n",
    "         [0],\n",
    "         [0],\n",
    "         [1]]\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "# W & b\n",
    "W = tf.Variable(tf.random_normal([2,1]),name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]),name=\"bias\")\n",
    "# H\n",
    "logit = tf.matmul(X,W)+b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# Cost \n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels = Y))\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _,cost_val = sess.run([train,cost], feed_dict={X:x_data,\n",
    "                                                   Y:y_data})\n",
    "    if step%3000==0:\n",
    "        print(\"cost값은:{}\".format(cost_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict\n",
    "predict = tf.cast(H>0.5,dtype=tf.float32)\n",
    "sess.run(predict, feed_dict={X:[[0,0]]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost값은:0.8159719109535217\n",
      "cost값은:0.7017427682876587\n",
      "cost값은:0.693906307220459\n",
      "cost값은:0.6932177543640137\n",
      "cost값은:0.6931538581848145\n",
      "cost값은:0.6931477785110474\n",
      "cost값은:0.6931472420692444\n",
      "cost값은:0.6931471824645996\n",
      "cost값은:0.6931471824645996\n",
      "cost값은:0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "# XOR 연산\n",
    "x_data = [[0,0],\n",
    "         [0,1],\n",
    "         [1,0],\n",
    "         [1,1]]\n",
    "y_data = [[0],\n",
    "         [1],\n",
    "         [1],\n",
    "         [0]]\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "# W & b\n",
    "W = tf.Variable(tf.random_normal([2,1]),name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]),name=\"bias\")\n",
    "# H\n",
    "logit = tf.matmul(X,W)+b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# Cost \n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels = Y))\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _,cost_val = sess.run([train,cost], feed_dict={X:x_data,\n",
    "                                                   Y:y_data})\n",
    "    if step%3000==0:\n",
    "        print(\"cost값은:{}\".format(cost_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict\n",
    "predict = tf.cast(H>0.5,dtype=tf.float32)\n",
    "sess.run(predict, feed_dict={X:[[0,0]]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "#### Perceptron(logistic)으로 AND/OR은 구현 가능, XOR(Exclusive OR)는 구현 불가!\n",
    "- 많은 사람들이 XOR를 어떻게 Perceptron으로 구현할 수 있을까? 고민 \n",
    "\n",
    "#### 1969에 마빈 민스키라는 사람이 논문 하나 발표\n",
    "- MIT AI lab 창시자 \n",
    "- XOR는 한 개의 perceptron으로 학습이 불가능하다는 것을 수학적으로 증명\n",
    "- MLP(multi layer perceptron)으로는 가능하다\n",
    "- MLP는 학습이 너무 어려워서 지구상에 있는 누구라도 이 학습을 시킬 수 없음 \n",
    "    - ->1970년대 AI의 침체기\n",
    "\n",
    "#### 1974년도 Paul이라는 박사과정 학생이 Backpropagation 방법을 고안 \n",
    "- 그러나 이미 배는 떠났음.. 아무도 안함... 묻히게 됨\n",
    "- 1982년도에 다시 한 번 논문을 발표\n",
    "\n",
    "#### 1986년도에 Hinton 교수가 논문 발표(Backpropagation 내용 가지고)(명망 높은 사람이라 사람들이 관심 가지기 시작)\n",
    "- AI가 다시 각광받기 시작\n",
    "\n",
    "#### 1995년쯤에 Backpropagation 방식이 안되는 건 아닌데 복잡한 문제는 해결 불가능하다.\n",
    "- 이 시기에 다른 여러가지 알고리즘이 나타나기 시작\n",
    "- SVM, 나이브 베이지언, Decision Tree \n",
    "- LeCUN => 다른 알고리즘이 더 우수하다는 것을 증명 \n",
    "- 다시 침체기에 진입\n",
    "\n",
    "#### 1990년도쯤 캐나다가 국책 연구기관을 설립\n",
    "- Canadian Institute For Adanced Research(CIFAR)\n",
    "- 1987년에 Hinton교수가 캐나다로 건너가서 AI연구를 지속\n",
    "- 2006, 2007년도에 Hinton교수가 2개의 논문을 발표 \n",
    "    - 망했던 이유를 수학적으로 증명함 \n",
    "    - 2006년도 => 이유: W와 b의 초기값을 randomㅡ로 주면 안된다는걸 알아냄.\n",
    "    - 2007년도 => 초기값 증명에 대한 논문. layer를 더 많이 사용할수록 복잡한 문제를 해결할 수 있다는 논문 발표\n",
    "- 사람들의 반응이 차가움..양치기 소년...\n",
    "- 신분세탁(rebranding) => Deep Learning\n",
    "\n",
    "#### 가장 많이 쓰이는 분야: 추천서비스\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2개짜리 logistic (deep learning)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR 연산\n",
    "x_data = [[0,0],\n",
    "         [0,1],\n",
    "         [1,0],\n",
    "         [1,1]]\n",
    "y_data = [[0],\n",
    "         [1],\n",
    "         [1],\n",
    "         [0]]\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# W & b & layer\n",
    "W1 = tf.Variable(tf.random_normal([2,2]),name=\"weight1\")  # 앞쪽logistic  # 뒤의 logistic이 계산 수행하려면 x 두개 수행해야 하므로 결과 2개 출력해줘야 함                                                    \n",
    "b1 = tf.Variable(tf.random_normal([2]),name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2,1]),name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]),name=\"bias2\")\n",
    "\n",
    "# H\n",
    "logit = tf.matmul(layer1,W2)+b2\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# Cost \n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels = Y))\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _,cost_val = sess.run([train,cost], feed_dict={X:x_data,\n",
    "                                                   Y:y_data})\n",
    "    if step%3000==0:\n",
    "        print(\"cost값은:{}\".format(cost_val))\n",
    "\n",
    "# predict\n",
    "predict = tf.cast(H>0.5,dtype=tf.float32)\n",
    "sess.run(predict, feed_dict={X:[[1,1]]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layer를 늘려보자! - 다계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR 연산\n",
    "x_data = [[0,0],\n",
    "         [0,1],\n",
    "         [1,0],\n",
    "         [1,1]]\n",
    "y_data = [[0],\n",
    "         [1],\n",
    "         [1],\n",
    "         [0]]\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# W & b & layer - 100개,256개의 logistic\n",
    "W1 = tf.Variable(tf.random_normal([2,100]),name=\"weight1\")  # 앞쪽logistic  # 뒤의 logistic이 계산 수행하려면 x 두개 수행해야 하므로 결과 2개 출력해줘야 함                                                    \n",
    "b1 = tf.Variable(tf.random_normal([100]),name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([100,256]),name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([256]),name=\"bias2\")\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256,10]),name=\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([10]),name=\"bias3\")\n",
    "# H\n",
    "logit = tf.matmul(layer2,W3)+b3\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# Cost \n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels = Y))\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _,cost_val = sess.run([train,cost], feed_dict={X:x_data,\n",
    "                                                   Y:y_data})\n",
    "    if step%3000==0:\n",
    "        print(\"cost값은:{}\".format(cost_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with Neural Network(Deep Learning)\n",
    "### tensorflow가 기본으로 제공하는 예제를 이용해서 구현해보자! \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data \n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")  # warning 출력 X\n",
    "\n",
    "# data loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:0.8594922423362732\n",
      "cost:0.34364792704582214\n",
      "cost:0.3790796399116516\n",
      "cost:0.2514866888523102\n",
      "cost:0.07994160056114197\n",
      "cost:0.313323974609375\n",
      "cost:0.1302802413702011\n",
      "cost:0.0744534432888031\n",
      "cost:0.30898866057395935\n",
      "cost:0.08133385330438614\n"
     ]
    }
   ],
   "source": [
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)  # 28*28 = 784\n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)  # 0-9\n",
    "\n",
    "# W & b (Deep & wide)  - depth가 길면 길수록 정확한 값 안나오고 시간이 너무 오래 걸리므로 적절한 크기로 해줘야 한다. \n",
    "W1 = tf.Variable(tf.random_normal([784,256]),name=\"weight1\") \n",
    "b1 = tf.Variable(tf.random_normal([256]),name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256,256]),name=\"weight2\") \n",
    "b2 = tf.Variable(tf.random_normal([256]),name=\"bias2\")\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256,10]),name=\"weight3\") \n",
    "b3 = tf.Variable(tf.random_normal([10]),name=\"bias3\")\n",
    "logit = tf.matmul(layer2,W3)+b3\n",
    "H = tf.nn.softmax(logit)  # 사실 sigmoid 쓰나 softmax쓰나 같은 결과. softmax는 전체 확률로 바뀌고 sigmoid에서 가장 큰 값이 softmax에서도 가장 큰 값인건 바뀌지 않음\n",
    "\n",
    "# cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,labels=Y))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "num_of_epoch = 30  # 반복횟수\n",
    "batch_size = 100\n",
    "\n",
    "for step in range(num_of_epoch): \n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)\n",
    "    cost_val = 0\n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)  # x,y 100개씩 떼어오기\n",
    "        _, cost_val = sess.run([train,cost],feed_dict = {X:batch_x,\n",
    "                                                         Y:batch_y})\n",
    "    if step%3==0:\n",
    "        print(\"cost:{}\".format(cost_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도:0.9185000061988831\n"
     ]
    }
   ],
   "source": [
    "# accuracy 측정\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "print(\"정확도:{}\".format(sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                                       Y:mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 생각보다 정확도가 많이 향상되지 않았음 \n",
    "- Hinton이 그 원인을 파악하려고 노력 \n",
    "- deep learning은 조금 더 학습이 잘 되기 위해 layer를 추가하고 각 layer에 많은 perceptron을 추가해서 구현 \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:11.322698593139648\n",
      "cost:1.0717881917953491\n",
      "cost:2.0749340057373047\n",
      "cost:4.799790382385254\n",
      "cost:3.070990800857544\n",
      "cost:1.8406081199645996\n",
      "cost:2.491458985787176e-07\n",
      "cost:0.0007727353950031102\n",
      "cost:0.004084933549165726\n",
      "cost:0.00621586199849844\n"
     ]
    }
   ],
   "source": [
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)  # 28*28 = 784\n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)  # 0-9\n",
    "\n",
    "# W & b (Deep & wide)  - depth가 길면 길수록 정확한 값 안나오고 시간이 너무 오래 걸리므로 적절한 크기로 해줘야 한다. \n",
    "W1 = tf.Variable(tf.random_normal([784,256]),name=\"weight1\") \n",
    "b1 = tf.Variable(tf.random_normal([256]),name=\"bias1\")\n",
    "layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256,256]),name=\"weight2\") \n",
    "b2 = tf.Variable(tf.random_normal([256]),name=\"bias2\")\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256,10]),name=\"weight3\") \n",
    "b3 = tf.Variable(tf.random_normal([10]),name=\"bias3\")\n",
    "logit = tf.matmul(layer2,W3)+b3\n",
    "H = tf.nn.relu(logit)  # 사실 sigmoid 쓰나 softmax쓰나 같은 결과. softmax는 전체 확률로 바뀌고 sigmoid에서 가장 큰 값이 softmax에서도 가장 큰 값인건 바뀌지 않음\n",
    "\n",
    "# cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,labels=Y))  # cost는 그대로\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "num_of_epoch = 30  # 반복횟수\n",
    "batch_size = 100\n",
    "\n",
    "for step in range(num_of_epoch): \n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)\n",
    "    cost_val = 0\n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)  # x,y 100개씩 떼어오기\n",
    "        _, cost_val = sess.run([train,cost],feed_dict = {X:batch_x,\n",
    "                                                         Y:batch_y})\n",
    "    if step%3==0:\n",
    "        print(\"cost:{}\".format(cost_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도:0.9380999803543091\n"
     ]
    }
   ],
   "source": [
    "# accuracy 측정\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "print(\"정확도:{}\".format(sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                                       Y:mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinton 교수님이 중요하게 여기는 또 하나의 요건은 w의 초기값\n",
    "- 초기에는 RBM이라는 방법을 이용해서 초기화를 진행\n",
    "- 2010년도에 Xavier 초기화라는 방식이 논문으로 발표됨\n",
    "- 2015년에는 He's 초기화라는 방식이 논문으로 발표\n",
    "- 현재도 계속 연구가 진행되고 있음 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w의 초기값 지정\n",
    "W1 = tf.Variable(tf.random_normal([784,256]),name=\"weight1\") \n",
    "W1 = tf.get_variable(\"weight1\", \n",
    "                     shape=[784,256], \n",
    "                     initializer = tf.contrib.layers.xavier_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:0.06252000480890274\n",
      "cost:0.2053268551826477\n",
      "cost:0.062043942511081696\n",
      "cost:0.11786851286888123\n",
      "cost:0.03743182495236397\n",
      "cost:0.07991869002580643\n",
      "cost:0.032230619341135025\n",
      "cost:0.05960353836417198\n",
      "cost:0.004350653383880854\n",
      "cost:0.013502877205610275\n"
     ]
    }
   ],
   "source": [
    "# 그래프를 초기화해보자! \n",
    "tf.reset_default_graph()  #  이전에는 w를 random하게 주어서 오류 안떴었는데 이제 초기값 지정해주기때문에 다시 실행하면 오류뜸\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)  # 28*28 = 784\n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)  # 0-9\n",
    "\n",
    "# W & b (Deep & wide)  - depth가 길면 길수록 정확한 값 안나오고 시간이 너무 오래 걸리므로 적절한 크기로 해줘야 한다. \n",
    "\n",
    "W1 = tf.get_variable(\"weight1\", \n",
    "                     shape=[784,256], \n",
    "                     initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]),name=\"bias1\")\n",
    "layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\", \n",
    "                     shape=[256,256], \n",
    "                     initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]),name=\"bias2\")\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\", \n",
    "                     shape=[256,10], \n",
    "                     initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]),name=\"bias3\")\n",
    "logit = tf.matmul(layer2,W3)+b3\n",
    "H = tf.nn.relu(logit)  # 사실 sigmoid 쓰나 softmax쓰나 같은 결과. softmax는 전체 확률로 바뀌고 sigmoid에서 가장 큰 값이 softmax에서도 가장 큰 값인건 바뀌지 않음\n",
    "\n",
    "# cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,labels=Y))  # cost는 그대로\n",
    "\n",
    "# train\n",
    "#train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)  # 이거도 자주쓰임. 크게 성능의 향상 보기엔 어렵지만 좀더 성능 좋다는게 일반적인 생각\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "num_of_epoch = 50  # 반복횟수\n",
    "batch_size = 100\n",
    "\n",
    "for step in range(num_of_epoch): \n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)\n",
    "    cost_val = 0\n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)  # x,y 100개씩 떼어오기\n",
    "        _, cost_val = sess.run([train,cost],feed_dict = {X:batch_x,\n",
    "                                                         Y:batch_y})\n",
    "    if step%5==0:\n",
    "        print(\"cost:{}\".format(cost_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도:0.9678000211715698\n"
     ]
    }
   ],
   "source": [
    "# accuracy 측정\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "print(\"정확도:{}\".format(sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                                       Y:mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overfitting(과적합)\n",
    "\n",
    "- 학습한 모델이 training data set에 최적화 되어있는 상태\n",
    "- 테스트 데이터에는 잘 들어맞지 않는 상태를 지칭\n",
    "\n",
    "- 학습한 모델이 training data set에는 약 98% 이상 정확도를 가지고, test data set에 대해서는 85%정도 수준으로 정확도가 나오면 overfitting되었다고 한다.\n",
    "\n",
    "\n",
    "1. 일단 학습하는 데이터 수가 많아야 한다.  \n",
    "2. 필요없는 feature들은 학습에서 제외  \n",
    "   중복되는 feature들은 단일화시켜야 한다.\n",
    "3. 학습하는 과정에서 overfitting을 피할 수 있다.  \n",
    "   2014년도에 논문이 나온다.   \n",
    "   dropout 방식이라고 한다.   \n",
    "   함수 형태로 제공한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:0.2761344611644745\n",
      "cost:0.39639368653297424\n",
      "cost:0.23558546602725983\n",
      "cost:0.13816842436790466\n",
      "cost:0.3645763397216797\n",
      "cost:0.37570643424987793\n",
      "cost:0.1362086981534958\n",
      "cost:0.2663983106613159\n",
      "cost:0.10393310338258743\n",
      "cost:0.1957578808069229\n"
     ]
    }
   ],
   "source": [
    "# 그래프를 초기화해보자! \n",
    "tf.reset_default_graph() \n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)  # 28*28 = 784\n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)  # 0-9\n",
    "dout_rate = tf.placeholder(dtype=tf.float32) # 값 1개 scalar값이므로 shape 안줘도 된다. -> drop out 비율\n",
    "\n",
    "# W & b (Deep & wide) \n",
    "W1 = tf.get_variable(\"weight1\", \n",
    "                     shape=[784,256], \n",
    "                     initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]),name=\"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "layer1 = tf.nn.dropout(_layer1,rate=dout_rate) # 256개  output을 다 뽑아내지 않겠다. node를 아예 삭제하는게 아니라 기능을 상실시키는 것\n",
    "                                         # rate=0 하면 다 살아있는거   0.3하면 30% 죽이는거 -> 다음으로 30프로 죽여서 넘기겠다.\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\", \n",
    "                     shape=[256,256], \n",
    "                     initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]),name=\"bias2\")\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "layer2 = tf.nn.dropout(_layer2,rate=dout_rate) \n",
    "\n",
    "W3 = tf.get_variable(\"weight3\", \n",
    "                     shape=[256,10], \n",
    "                     initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]),name=\"bias3\")  # 맨 마지막에는 dropout해주지 않는다. -> 다 넘겨야 하므로\n",
    "logit = tf.matmul(layer2,W3)+b3\n",
    "H = tf.nn.relu(logit)  \n",
    "\n",
    "# cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,labels=Y))  \n",
    "\n",
    "# train\n",
    "#train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)  # 이거도 자주쓰임. 크게 성능의 향상 보기엔 어렵지만 좀더 성능 좋다는게 일반적인 생각\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "num_of_epoch = 50  # 반복횟수\n",
    "batch_size = 100\n",
    "\n",
    "for step in range(num_of_epoch): \n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)\n",
    "    cost_val = 0\n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)  # x,y 100개씩 떼어오기\n",
    "        _, cost_val = sess.run([train,cost],feed_dict = {X:batch_x,\n",
    "                                                         Y:batch_y,\n",
    "                                                         dout_rate:0.3}) # 30% 끄고 학습해라\n",
    "    if step%5==0:\n",
    "        print(\"cost:{}\".format(cost_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도:0.9689000248908997\n"
     ]
    }
   ],
   "source": [
    "# accuracy 측정\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "print(\"정확도:{}\".format(sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                                       Y:mnist.test.labels,\n",
    "                                                       dout_rate:0})))  # test때는 모든 애들을 다 켜고 테스트해야 하므로 0을 죽인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU_ENV]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
